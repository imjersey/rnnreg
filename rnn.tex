
% Use the following line  only  if you're still using LaTeX 2.09.
%\documentstyle[icml2014,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}
% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 
\usepackage{python}
\usepackage{footnote}
\usepackage{tablefootnote}


% For citations
\usepackage{natbib}
\usepackage{comment}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}


% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
% \usepackage[nohyperref]{icml2014} above.
\usepackage{hyperref}
\usepackage{epic,eepic}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
%\usepackage{icml2014} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
\usepackage[accepted]{icml2014}



\begin{document} 

\twocolumn[
\icmltitle{Recurrent neural network regularization}

\icmlauthor{Wojciech Zaremba}{woj.zaremba@gmail.com}
\icmladdress{Google \& New York University}
\vskip -0.08in
\icmlauthor{Ilya Sutskever}{ilyasu@google.com}
\icmladdress{Google}
\vskip -0.08in
\icmlauthor{Oriol Vinyals}{vinyals@google.com}
\icmladdress{Google}


\icmlkeywords{natual language processing, recurrent neural networks, language model, LSTMs, speech recognition, machine translation}

\vskip 0.3in
]

\begin{abstract} 
  We present a simple regularization technique of recurrent neural networks (RNNs)
  with long short term memory (LSTM) units.
  This technique is based on dropout and gives tremendous performance boost.
  We show that it is beneficial in variety sequence modelling problems like
  language modeling, speech recognition, and machine translation.

\end{abstract} 

\section{Introduction}
Recurrent neural networks yields the state-of-the-art performance on many sequence modelling
tasks like language modelling, and speech recognition. Moreover, recent results in 
machine translation \cite{cho2014learning} shows their potential use in this field as well. 
However, up today there was no good techniques to regularize them. 
Various attempts to inject noise or mask some of activations (dropout) were 
giving small improvement in compare to model averaging. This work presents
how to augment LSTMs with dropout. Resulting models give a tremendous 
improvement in various domains.


\section{Related work}

\cite{hochreiter1997long}

\section{Regularized RNN with LSTMs}

\subsection{Long-short term memory units}


\begin{figure}
\begin{align*}
&\begin{pmatrix}i\\f\\o\\g\end{pmatrix} =
  \begin{pmatrix}\sigma\\\sigma\\\sigma\\\tanh\end{pmatrix}
  L\begin{pmatrix}{\bf D}(h^{l - 1}_k)\\h^l_{k-1}\end{pmatrix}\\
&c^l_k = f \odot c^l_{k-1} + i \odot g\\
&h^l_k = o \odot \tanh(c_k)\\
\end{align*}
\caption{To describe dynamics of multilayer LSTM with dropout, we use multi-indexing. Lower indices correspond to dynamics over time, and upper indices correspond to dynamics over layers. For simplicity, we denote by $L$ a linear transform with bias ($Wx + b$), and by $D$ a dropout layer. $\odot$ is a element-wise multiplication. $h^0_k$ is a input word-vector, and $h^{L}_k$ is used to predict $y_k$ ($L$ is a number of layers).}
\end{figure}




\subsection{Regularization with dropout}

\begin{figure}
  \begin{center}
    \begin{picture}(150, 200)
      \multiput(0,0)(35, 0){6}{
        \put(-25, 45){\vector(1, 0){25}}
        \put(-25, 100){\vector(1, 0){25}}
      }
      \multiput(0,0)(35, 0){5}{
        \put(0, 0){
          \put(0, 85){\framebox(10, 30){}}
          \put(0, 30){\framebox(10, 30){}}
          \multiput(0,0)(0, 5){4}{
            \put(5, 7){\line(0, 0){2}}
            \put(5, 60){\line(0, 0){2}}
            \put(5, 115){\line(0, 0){2}}
          }
          \put(5, 30){\vector(0, 0){0.1}}
          \put(5, 85){\vector(0, 0){0.1}}
          \put(5, 138){\vector(0, 0){0.1}}
        }
      }
      \put(-2, 0){\makebox{$x_{i-2}$}}
      \put(33, 0){\makebox{$x_{i-1}$}}
      \put(71, 0){\makebox{$x_{i}$}}
      \put(103, 0){\makebox{$x_{i+1}$}}
      \put(138, 0){\makebox{$x_{i+2}$}}
      \put(-2, 142){\makebox{$y_{i-2}$}}
      \put(33, 142){\makebox{$y_{i-1}$}}
      \put(71, 142){\makebox{$y_{i}$}}
      \put(103, 142){\makebox{$y_{i+1}$}}
      \put(138, 142){\makebox{$y_{i+2}$}}
    \end{picture}
  \end{center}
  \caption{Regularized multilayer RNN. Dashed arrows indicate connections with applied dropout, while
  solid lines indicate connections where dropout is not applied.}
\end{figure}

\subsection{Intuition}
We will describe here justifications why putting dropout across recurrent layers helps,
but within layers degrades performance. 


Dropout removes part of information, and LSTMs has to become
more robust while performing input-output mapping. Simultaneously we don't want to erase
entire information. Especially we would like to facilitate memory about event that happened
long time ago. Figure \ref{fig:flow} shows flow of an exemplary information from the 
event $x_{i-2}$ to the prediction in the step $i+2$. We can see that information is projected
with dropout only $L$ times, and it is independent of how far in past event occurred. All
the previous regularization techniques were constraining recurrent connections, which 
effectively exponentially fast ``blurred'' information about the past.



\begin{figure}
  \begin{center}
    \begin{picture}(150, 200)
      \multiput(0,0)(35, 0){6}{
        \put(-25, 45){\vector(1, 0){25}}
        \put(-25, 100){\vector(1, 0){25}}
      }
      \multiput(0,0)(35, 0){5}{
        \put(0, 0){
          \put(0, 85){\framebox(10, 30){}}
          \put(0, 30){\framebox(10, 30){}}
          \multiput(0,0)(0, 5){4}{
            \put(5, 7){\line(0, 0){2}}
            \put(5, 60){\line(0, 0){2}}
            \put(5, 115){\line(0, 0){2}}
          }
          \put(5, 30){\vector(0, 0){0.1}}
          \put(5, 85){\vector(0, 0){0.1}}
          \put(5, 138){\vector(0, 0){0.1}}
        }
      }
      \put(-2, 0){\makebox{$x_{i-2}$}}
      \put(33, 0){\makebox{$x_{i-1}$}}
      \put(71, 0){\makebox{$x_{i}$}}
      \put(103, 0){\makebox{$x_{i+1}$}}
      \put(138, 0){\makebox{$x_{i+2}$}}
      \put(-2, 142){\makebox{$y_{i-2}$}}
      \put(33, 142){\makebox{$y_{i-1}$}}
      \put(71, 142){\makebox{$y_{i}$}}
      \put(103, 142){\makebox{$y_{i+1}$}}
      \put(138, 142){\makebox{$y_{i+2}$}}

       
      {\linethickness{0.6mm}
        \put(5, 7){\line(0, 0){38}}
        \put(4, 45){\line(1, 0){105}}
        \put(110, 44){\line(0, 0){56}}
        \put(109, 100){\line(1, 0){35}}
        \put(145, 99){\line(0, 0){35}}
      }
    \end{picture}
  \end{center}
  \caption{Thick line indicates an exemplary information flow in RNN. Information flow line is crossed $L$ times, where $L$ is depth of network.}
  \label{fig:flow}
\end{figure}


\section{Experiments}

\subsection{Language modeling}

Our model is a two layer LSTM network, with 650 units initialized to uniformly in
$[-0.05, 0.05]$. We apply $50\%$ of dropout on non-recurrent connections. We train for
$39$ epochs, starting with learning rate $1$, and after $6$ epochs we decrease it by $1.2$ 
in every epoch. We unroll RNN for $35$ steps, and clip gradients at $5$. 

\begin{table}[t]
  \small
  \centering
  \renewcommand{\arraystretch}{1.15}
  \begin{tabular}{lll}
    \hline
     Model & Validation set & Test set \\
    \hline
    \multicolumn{3}{c}{A single model} \\
    \hline
    Previous state-of-the-art \tablefootnote{\cite{pascanu2013construct}} & & 107.5 \\
    Regularized LSTM & {\bf 86.8} & {\bf 82.7} \\
    \hline
    \multicolumn{3}{c}{Model averaging} \\
    \hline
    Previous state-of-the-art \tablefootnote{\cite{mikolov2012statistical}} & 83.5\tablefootnote{Weight of individual models are tuned to minimize this score. This few parameters are fit on this validation set, which is not completely fair.} & 89.4 \\
    2 regularized LSTMs & & \\
    5 regularized LSTMs & & \\
    10 regularized LSTMs & & \\
    \hline
  \end{tabular}
  \caption{Word-level perplexity on Penn-tree-bank dataset.}
\end{table}


\subsection{Machine translation}

\subsection{Speech recognition}

\section{Discussion}

\bibliography{bibliography}
\bibliographystyle{icml2014}

\end{document} 

