
% Use the following line  only  if you're still using LaTeX 2.09.
%\documentstyle[icml2014,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}
% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 
\usepackage{footnote}
\usepackage{amsfonts}

% For citations
\usepackage{natbib}
\usepackage{comment}
\usepackage{multirow}


% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}

\setlength{\abovedisplayskip}{0cm}
\setlength{\belowdisplayskip}{0cm}

\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{0.5ex}{0.3ex}
\titlespacing{\subsection}{0pt}{0.2ex}{0ex}
\titlespacing{\subsubsection}{0pt}{0.1ex}{0ex}

\newcommand{\startcompact}[1]{\par\vspace{-0.75em}\begin{#1}%
  \allowdisplaybreaks\ignorespaces}

\newcommand{\stopcompact}[1]{\end{#1}\ignorespaces}

\usepackage{paralist}

\makeatletter
\ifcase \@ptsize \relax% 10pt
  \newcommand{\miniscule}{\@setfontsize\miniscule{4}{5}}% \tiny: 5/6
\or% 11pt
  \newcommand{\miniscule}{\@setfontsize\miniscule{5}{6}}% \tiny: 6/7
\or% 12pt
  \newcommand{\miniscule}{\@setfontsize\miniscule{5}{6}}% \tiny: 6/7
\fi
\makeatother

\newcommand {\aplt} {\ {\raise-.5ex\hbox{$\buildrel<\over\sim$}}\ }

\newcommand{\eqn}[1]{Eqn.~\ref{eqn:#1}}
\newcommand{\fig}[1]{Fig.~\ref{fig:#1}}
\newcommand{\tab}[1]{Table~\ref{tab:#1}}
\newcommand{\secc}[1]{Section~\ref{sec:#1}}
\def\etal{{\textit{et~al.~}}}
\newcommand{\BigO}[1]{\ensuremath{\operatorname{O}\left(#1\right)}}
\usepackage[symbol*]{footmisc}

\DefineFNsymbolsTM{myfnsymbols}{% def. from footmisc.sty "bringhurst" symbols
  \textasteriskcentered *
  \textdagger    \dagger
  \textdaggerdbl \ddagger
  \textsection   \mathsection
  \textbardbl    \|%
  \textparagraph \mathparagraph
}%


% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
% \usepackage[nohyperref]{icml2014} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
%\usepackage{icml2014} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
\usepackage[accepted]{icml2014}

\begin{document} 

\twocolumn[
\icmltitle{Recurrent neural network regularization}

\icmlauthor{Wojciech Zaremba}{woj.zaremba@gmail.com}
\vskip -0.03in
\icmladdress{Google \& New York University}
\vskip -0.08in
\icmlauthor{Ilya Sutskever}{ilyasu@google.com}
\vskip -0.03in
\icmladdress{Google}
\vskip -0.08in
\icmlauthor{Oriol Vinyals}{vinyals@google.com}
\vskip -0.03in
\icmladdress{Google}


\icmlkeywords{natual language processing, recurrent neural networks, language model, LSTMs, speech recognition, machine translation}

\vskip 0.3in
]

\begin{abstract} 
  We present a simple regularization technique for Recurrent Neural
  Networks (RNNs) with Long Short-Term Memory (LSTM) units.  The
  technique is based on dropout and gives a tremendous reduction in
  overfitting.  We show that it is useful in a variety of sequence
  modelling problems that include language modeling, speech recognition, and
  machine translation.
\end{abstract} 

\section{Introduction}

RNNs yield the state of the art performance on many sequence modelling
tasks, including language modelling \cite{mikolov}, speech recognition
\cite{graves2013speech}, and machine translation \cite{SVL2014}.
However, there have been no good ways of regularizing them. As a
result, practical applications tend to use RNNs that are too small, as
large RNNs would overfit.  To date, existing regularization methods
give relatively small improvements on RNNs
\cite{graves2013generating}.  Dropout is a highly effective way of
regularizing feedforward neural networks
\cite{srivastava2013improving} that had enjoyed considerable
success. However, it is not clear how to use dropout on RNNs,
because a naive application of dropout does not yield good results.
In this work, we show how to correctly apply dropout to LSTMs, and
demonstrate that this results in great reduction in overfitting.

\section{Related work}

Dropout \cite{srivastava2013improving} is a recently regularization
method that has enjoyed a lot of success in applications of
feedforward neural networks.  While there has been a lot of work
extending Dropout \cite{wang2013fast, wan2013regularization}, there
has been relatively little research so far in applying dropout to
RNNs. The only paper on the topic is \cite{bayer2013fast} which
focuses on ``marginalized dropout'' (from \cite{wang2013fast}) which
is a noiseless deterministic approximation to conventional dropout.
\cite{bayer2013fast} claims that conventional dropout cannot be
successfully used in RNNs due to its large variance which hurts
learning and causes poor convergence. In this work, we show that a
specific application of dropout greatly reduces the overfitting of RNNs.

There has been extensive work on using RNNs for language modelling
\cite{mikolov2012statistical, sutskever2013training}. Moreover, there
have been a number of architectural variations on the RNN that are
better suited for learning on data with long term dependencies
\cite{hochreiter1997long, graves2009novel, cho2014learning,
  leaky_neurons, clockwork_rnn}.  This work focuses on the LSTM which
is the most widespread variant of the RNN, although it is likely that 
our findings are valid for other models.

In this paper, we focus on the following RNN tasks: language
modelling, speech recognition, and machine translation.  Language
modelling is the first task where RNNs achieved substantial success
\cite{mikolov2010recurrent, mikolov2011strategies,
  pascanu2013construct}.  RNNs have also been successfully used for
speech recognition \cite{robinson1996use, graves2013speech} and have
been applied to machine translation, where they are typically
used for language modelling, re-ranking, or phrase modelling
\cite{SVL2014,cho2014learning,bbn,other_neural_MT_papers}.

\section{Regularizing RNN with LSTM cells}

In this section we describe the deep LSTM \ref{sec:lstm}. Next, 
we show how to regularize them \ref{sec:reg}, and we provide an intuition
for why our regularization scheme works.

We use upperscript to denote a layer number and lowerscript to denote
a timestep.  All our states are $n$-dimensional.  Let $h^l_k \in
\mathbb{R}^{n}$ be a hidden state in layer $l$ in step $k$. Moreover,
let $T_{n,m}:\mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ be a linear
transform and with a bias ($Wx + b$ for some $W$ and $b$).  Let
$\odot$ be a element-wise multiplication and let $h^0_k$ be an input
word vector.  We use the activations $h^{L}_k$ to predict $y_k$, since
$L$ is the number of layers in our deep LSTM.

\subsection{Long-short term memory units}
\label{sec:lstm}

The RNNs dynamics can be described in terms of deterministic transitions
from previous hidden states to the hidden states in the next step. 
The transition is a function
\begin{align*}
  &\text{RNN} : h^{l-1}_k, h^l_{k-1} \rightarrow h^l_k
\end{align*}

For classical RNNs, this function is given by
\begin{align*}
  h^l_k = f(T_{n,n}h^{l-1}_k + T_{n,n}h^l_{k-1}) \text{, where $f \in \{\mathrm{sigm}, \tanh\}$ }
\end{align*}

The LSTM has relatively complicated dynamics that make it easy to
``memorize'' information for extended number of time steps.  The
``long term'' memory is stored in a vector of \emph{memory cells}
$c^l_k \in \mathbb{R}^n$.  Although there are many LSTM architectures
that differ in their connectivity sturcutre and activation functions,
all LSTM architectures have explicit memory cells that make it easy to
store information for extended periods of time.  The LSTM can decide
to overwrite this information, retrieve it, or keep it for the next time
step.  The LSTM architecture used in our experiments is given by the
following equations \cite{graves2013speech}:
\begin{align*}
&\text{LSTM} : h^{l-1}_k, h^l_{k-1}, c^l_{k - 1} \rightarrow h^l_k, c^l_k\\
&\begin{pmatrix}i\\f\\o\\g\end{pmatrix} =
  \begin{pmatrix}\mathrm{sigm}\\\mathrm{sigm}\\\mathrm{sigm}\\\tanh\end{pmatrix}
  T_{2n,4n}\begin{pmatrix}h^{l - 1}_k\\h^l_{k-1}\end{pmatrix}\\
&c^l_k = f \odot c^l_{k-1} + i \odot g\\
&h^l_k = o \odot \tanh(c^l_k)\\
\end{align*}
In these equations, $\mathrm{sigm}$ and $\tanh$ are applied
element-wise. Diagram \ref{fig:lstm} illustrates the LSTM
equations.


\begin{figure}
  \begin{center}
    \begin{picture}(200, 130)
      \put(0, 0){\framebox(180, 100){}}
      \put(90, 50){\circle{16}}
      \put(86.5, 48){$\mathbf c_t$}
      \put(84, 60){{\scriptsize Cell}}

      \put(90, 32){\circle{6.5}}
      \put(87.25, 30.5){{\tiny $\times$}}

      \put(90, 12){\circle{16}}
      \put(86.5, 10){{\small $f$}}
      \put(100, 10){{\scriptsize Forget gate}}

      \put(90, 20){\vector(0, 0){8}}

      \put(85, 44){\vector(2, 3){0}}
      \qbezier(86, 32)(80, 36.5)(83, 41)

      \qbezier(96, 34)(100, 36.5)(95.5, 43.5)
      \put(93.5, 31.5){\vector(-1, -1){0}}
      
      \put(82, -8){\vector(1, 2){6}}
      \put(72.5, -13){{\small $h_{k-1}^{l}$}}
      \put(98, -8){\vector(-1, 2){6}}
      \put(99, -13){{\small $h_{k}^{l-1}$}}

      \put(30, 87){\circle{16}}
      \put(28, 85){{\small $i$}}
      \put(6, 85){{\scriptsize $\begin{matrix}\text{Input}\\\text{gate}\end{matrix}$}}

      \put(21.5, 105){\vector(1, -2){5}}
      \put(12.5, 108){{\small $h_{k-1}^{l}$}}
      \put(39.5, 107){\vector(-1, -2){6}}
      \put(37, 108){{\small $h_{k}^{l-1}$}}

      \put(147, 87){\circle{16}}
      \put(144.5, 85){{\small $o$}}
      \put(156, 85){{\scriptsize $\begin{matrix}\text{Output}\\\text{gate}\end{matrix}$}}
        
      \put(138.5, 105){\vector(1, -2){5}}
      \put(129.5, 108){{\small $h_{k-1}^{l}$}}
      \put(156.5, 107){\vector(-1, -2){6}}
      \put(154, 108){{\small $h_{k}^{l-1}$}}

      \put(17, 50){\circle{16}}
      \put(15, 48){{\small $g$}}
      \put(1, 28){{\scriptsize $\begin{matrix}\text{Input}\\\text{modulation}\\\text{gate}\end{matrix}$}}

      \put(53.5, 50){\circle{6.5}}
      \put(50.75, 48.5){{\tiny $\times$}}

      \put(57, 50){\vector(1, 0){25}}
      \put(25, 50){\vector(1, 0){25}}
      \put(35, 80){\vector(2, -3){17.5}}

      \put(147, 50){\circle{6.5}}
      \put(144.25, 48.5){{\tiny $\times$}}
      \put(98, 50){\vector(1, 0){45.25}}
      \put(150.5, 50){\vector(1, 0){38}}

      \put(147, 79){\vector(0, -1){25.5}}
      \put(190, 47){${\mathbf h^l_k}$}


      \put(-20, 40){{\small $h_{k-1}^{l}$}}
      \put(-20, 56){{\small $h_{k}^{l-1}$}}
      \put(-10, 44){\vector(4, 1){19}}
      \put(-10, 58){\vector(4, -1){19}}


    \end{picture}
  \end{center}
  \caption{Graphical representation of LSTM memory cells used in this paper (there are minor differences in compare to \cite{graves2013generating}.}
  \label{fig:lstm}
\end{figure}


\subsection{Regularization with Dropout} 
\label{sec:reg}

The main contribution of this paper is the discovery that carefully
placed dropout tremendously improves the generalization ability of LSTMs.
To be effective, dropout must be placed on the non-recurrent connections
\ref{fig:reg}.  The following equation describes it more precisely,
where ${\bf D}$ is the dropout operator that sets a random subset of its 
argument to zero:

\begin{align*}
&\begin{pmatrix}i\\f\\o\\g\end{pmatrix} =
  \begin{pmatrix}\mathrm{sigm}\\\mathrm{sigm}\\\mathrm{sigm}\\\tanh\end{pmatrix}
  T_{2n,4n}\begin{pmatrix}{\bf D}(h^{l - 1}_k)\\h^l_{k-1}\end{pmatrix}\\
&c^l_k = f \odot c^l_{k-1} + i \odot g\\
&h^l_k = o \odot \tanh(c^l_k)\\
\end{align*}


\begin{figure}
  \begin{center}
    \begin{picture}(150, 200)
      \multiput(0,0)(35, 0){6}{
        \put(-25, 45){\vector(1, 0){25}}
        \put(-25, 100){\vector(1, 0){25}}
      }
      \multiput(0,0)(35, 0){5}{
        \put(0, 0){
          \put(0, 85){\framebox(10, 30){}}
          \put(0, 30){\framebox(10, 30){}}
          \multiput(0,0)(0, 5){4}{
            \put(5, 7){\line(0, 0){2}}
            \put(5, 60){\line(0, 0){2}}
            \put(5, 115){\line(0, 0){2}}
          }
          \put(5, 30){\vector(0, 0){0.1}}
          \put(5, 85){\vector(0, 0){0.1}}
          \put(5, 138){\vector(0, 0){0.1}}
        }
      }
      \put(-2, 0){\makebox{$x_{i-2}$}}
      \put(33, 0){\makebox{$x_{i-1}$}}
      \put(71, 0){\makebox{$x_{i}$}}
      \put(103, 0){\makebox{$x_{i+1}$}}
      \put(138, 0){\makebox{$x_{i+2}$}}
      \put(-2, 142){\makebox{$y_{i-2}$}}
      \put(33, 142){\makebox{$y_{i-1}$}}
      \put(71, 142){\makebox{$y_{i}$}}
      \put(103, 142){\makebox{$y_{i+1}$}}
      \put(138, 142){\makebox{$y_{i+2}$}}
    \end{picture}
  \end{center}
  \caption{Regularized multilayer RNN. Dashed arrows indicate connections with applied dropout, while
  solid lines indicate connections where dropout is not applied.}
  \label{fig:reg}
\end{figure}

The dropout operator corrupts the information carried by the units,
which forces them to perform their intermediate computations in a more
robust manner. At the same time, we do not want to erase all of the
information. We would especially like to remember events that occurred
many timeseps in the past. Figure \ref{fig:flow} shows a possible flow
of information from an event that occurred at $x_{i-2}$ to the
prediction in the step $i+2$. We can see that the information is
corrupted by dropout only $L + 1$ times, and it is independent of how
far in past event occurred.  Previous regularization techniques would
perturb the recurrent connections or the recurrent hidden state, which
would greatly reduce the LSTM's memory capacity.  By not using dropout
on the recurrent connections, the LSTM is able to get most of the benefit of dropout
without sacrifising its valuable ability to learn and store information
for long periods of time.


\begin{figure}
  \begin{center}
    \begin{picture}(150, 200)
      \multiput(0,0)(35, 0){6}{
        \put(-25, 45){\vector(1, 0){25}}
        \put(-25, 100){\vector(1, 0){25}}
      }
      \multiput(0,0)(35, 0){5}{
        \put(0, 0){
          \put(0, 85){\framebox(10, 30){}}
          \put(0, 30){\framebox(10, 30){}}
          \multiput(0,0)(0, 5){4}{
            \put(5, 7){\line(0, 0){2}}
            \put(5, 60){\line(0, 0){2}}
            \put(5, 115){\line(0, 0){2}}
          }
          \put(5, 30){\vector(0, 0){0.1}}
          \put(5, 85){\vector(0, 0){0.1}}
          \put(5, 138){\vector(0, 0){0.1}}
        }
      }
      \put(-2, 0){\makebox{$x_{i-2}$}}
      \put(33, 0){\makebox{$x_{i-1}$}}
      \put(71, 0){\makebox{$x_{i}$}}
      \put(103, 0){\makebox{$x_{i+1}$}}
      \put(138, 0){\makebox{$x_{i+2}$}}
      \put(-2, 142){\makebox{$y_{i-2}$}}
      \put(33, 142){\makebox{$y_{i-1}$}}
      \put(71, 142){\makebox{$y_{i}$}}
      \put(103, 142){\makebox{$y_{i+1}$}}
      \put(138, 142){\makebox{$y_{i+2}$}}

       
      {\linethickness{0.6mm}
        \put(5, 7){\line(0, 0){38}}
        \put(4, 45){\line(1, 0){105}}
        \put(110, 44){\line(0, 0){56}}
        \put(109, 100){\line(1, 0){35}}
        \put(145, 99){\line(0, 0){35}}
      }
    \end{picture}
  \end{center}
  \caption{Thick line indicates an exemplary information flow in RNN. Information flow line is crossed $L + 1$ times, where $L$ is depth of network.}
  \label{fig:flow}
\end{figure}


\section{Experiments}

We present here results in there domains: language modeling \ref{sec:lang}, 
speech recognition \ref{sec:speech}, and machine translation \ref{sec:trans}.

\subsection{Language modeling}
\label{sec:lang}

We have conducted word-level prediction experiments on Penn tree bank
(PTB) dataset \cite{ptb_citation}.  This dataset consists of $929$k
training words, $73$k validation words, and $82$k test words. It has
$10$k words in vocabulary.

Our model is a two-layer LSTM with 650 units {\bf [so 325 units per
    layer?]} whoes parameters are initialized uniformly in $[-0.05,
  0.05]$. We apply $50\%$ dropout on the non-recurrent connections. We
train for $39$ epochs, starting with learning rate of $1$, and after
$6$ epochs we decrease it by a factor of $1.2$ in every epoch. We
unroll the RNN for $35$ steps, and clip the norm of the gradients
(normalized by minibatch size) at $5$. We set mini-batch to
$20$. Table \ref{tab:ptb} compares various models with our models.

\begin{table}[t]
  \small
  \centering
  \renewcommand{\arraystretch}{1.15}
  \begin{tabular}{lll}
    \hline
     Model & Validation set & Test set \\
    \hline
    \multicolumn{3}{c}{A single model} \\
    \hline
    \cite{pascanu2013construct} & & 107.5 \\
    \cite{chenglanguage} & & 100.0 \\
    Non-regularized LSTM & 120.7 & 114.5 \\
    Regularized LSTM & 86.2 & {\bf 82.7} \\
    \hline
    \multicolumn{3}{c}{Model averaging} \\
    \hline
    \cite{mikolov2012statistical} & 83.5 & 89.4 \\
    \cite{chenglanguage} & & 80.6 \\
    2 non-regularized LSTMs & 100.4 & 96.1 \\
    5 non-regularized LSTMs & 87.9 & 84.1 \\
    10 non-regularized LSTMs & 83.5 & 80.0 \\
    2 regularized LSTMs & 80.6 & 77.0 \\
    5 regularized LSTMs & 76.7 & 73.3 \\
    10 regularized LSTMs & 75.2 & {\bf 72.0} \\
    \hline
  \end{tabular}
  \caption{Word-level perplexity on Penn-tree-bank dataset.}
  \label{tab:ptb}
\end{table}

\subsection{Speech recognition}
\label{sec:speech}

\subsection{Machine translation}
\label{sec:trans}

\section{Discussion}
We present amazing performance boosts with methods, which we 
only intuitively understand. We think, that it crucial
to derive our models analytically, rather than based only on
intuition. However, so far this problems seem to be very difficult to
tackle.


\bibliography{bibliography}
\bibliographystyle{icml2014}

\end{document} 

