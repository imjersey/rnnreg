
% Use the following line  only  if you're still using LaTeX 2.09.
%\documentstyle[icml2014,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}
% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 
\usepackage{python}
\usepackage{footnote}
\usepackage{tablefootnote}
\usepackage{amsfonts}


% For citations
\usepackage{natbib}
\usepackage{comment}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}


% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
% \usepackage[nohyperref]{icml2014} above.
\usepackage{hyperref}
\usepackage{epic,eepic}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
%\usepackage{icml2014} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
\usepackage[accepted]{icml2014}



\begin{document} 

\twocolumn[
\icmltitle{Recurrent neural network regularization}

\icmlauthor{Wojciech Zaremba}{woj.zaremba@gmail.com}
\vskip -0.03in
\icmladdress{Google \& New York University}
\vskip -0.08in
\icmlauthor{Ilya Sutskever}{ilyasu@google.com}
\vskip -0.03in
\icmladdress{Google}
\vskip -0.08in
\icmlauthor{Oriol Vinyals}{vinyals@google.com}
\vskip -0.03in
\icmladdress{Google}


\icmlkeywords{natual language processing, recurrent neural networks, language model, LSTMs, speech recognition, machine translation}

\vskip 0.3in
]

\begin{abstract} 
  We present a simple regularization technique of recurrent neural networks (RNNs)
  with long short term memory (LSTM) units.
  This technique is based on dropout and gives tremendous performance boost.
  We show that it is beneficial in variety sequence modelling problems like
  language modeling, speech recognition, and machine translation.

\end{abstract} 

\section{Introduction}
RNNs yields the state-of-the-art performance on many sequence modelling
tasks like language modelling, and speech recognition. Moreover, recent results in 
machine translation \cite{cho2014learning} shows their potential use in this field as well. 
However, up today there was no good techniques to regularize them. 
Various attempts to inject noise or mask some of activations (dropout) were 
giving just a small improvement \cite{graves2013generating}. This work presents
how to augment LSTMs with dropout. Resulting models give a tremendous 
improvement over the baseline.




\section{Related work}
There have been extensive work on using RNNs for sequence modelling
\cite{mikolov2012statistical, sutskever2013training}. Moreover, there were
considered various twists into common architecture, which 
potentially can capture long term dependencies \cite{hochreiter1997long, graves2009novel, cho2014learning}. This work choses LSTM architecture, however it is likely that conclusions 
presented here would extend to other models. 



We focus here on tasks, which were previously considered in RNN literature like (1)
language modelling, (2) speech recognition, and (3) machine translation. 
Language modelling is a classical task tackled by
RNNs \cite{pascanu2013construct, mikolov2010recurrent, mikolov2011strategies}.
Using RNNs for speech recognition was previously 
considered by \cite{robinson1996use, graves2013speech}. Finally, machine translation
is a novel task for which RNNs are used. Usually, they are used for language modelling, 
re-ranking, or phrase modelling \cite{cho2014learning}.

\section{Regularized RNN with LSTMs}

We describe in this section how LSTMs work \ref{sec:lstm}. Next, 
we show how to regularize them \ref{sec:reg}, and we give some intuitions
why it works.


We use upper indexing for layer number, lower indexing for a time step. 
Our states are $n$ dimensional.
$h^l_k \in \mathbb{R}^{n}$ is a hidden state in layer $l$ in step $k$. Moreover, for simplicity
we denote by $T:\mathbb{R}^{n \times n} \rightarrow \mathbb{R}^{n}$ a 
linear transform with bias ($Wx + b$).
$\odot$ is a element-wise multiplication. $h^0_k$ is a input word-vector, 
and $h^{L}_k$ is used to predict $y_k$ ($L$ is a number of layers).


\subsection{Long-short term memory units}
\label{sec:lstm}

Dynamics of RNNs can be described in terms of transitions
from previous hidden states to the hidden states in the next step. 
That is the function 
\begin{align*}
  &\text{RNN} : h^{l-1}_k, h^l_{k-1} \rightarrow h^l_k
\end{align*}

In case of classical RNNs this function is described as: 
\begin{align*}
  h^l_k = f(Th^{l-1}_k + Lh^l_{k-1})
\end{align*}

LSTM has more complicated dynamics in order to be able to 
``memorize'' information for extended number of time steps. 
Memory is stored in cell ($c^l_k \in \mathbb{R}^n$) units. LSTM performs following mapping.
\begin{align*}
\text{LSTM} : h^{l-1}_k, h^l_{k-1}, c^l_{k - 1} \rightarrow h^l_k, c^l_k
\end{align*}

This mapping is described by:
\begin{align*}
&\begin{pmatrix}i\\f\\o\\g\end{pmatrix} =
  \begin{pmatrix}\sigma\\\sigma\\\sigma\\\tanh\end{pmatrix}
  T\begin{pmatrix}h^{l - 1}_k\\h^l_{k-1}\end{pmatrix}\\
&c^l_k = f \odot c^l_{k-1} + i \odot g\\
&h^l_k = o \odot \tanh(c^l_k)\\
\end{align*}
Where $\sigma, \tanh$ are applied element-wise.



\subsection{Regularization with dropout} 
\label{sec:reg}



\begin{align*}
&\begin{pmatrix}i\\f\\o\\g\end{pmatrix} =
  \begin{pmatrix}\sigma\\\sigma\\\sigma\\\tanh\end{pmatrix}
  T\begin{pmatrix}{\bf D}(h^{l - 1}_k)\\h^l_{k-1}\end{pmatrix}\\
&c^l_k = f \odot c^l_{k-1} + i \odot g\\
&h^l_k = o \odot \tanh(c^l_k)\\
\end{align*}


\begin{figure}
  \begin{center}
    \begin{picture}(150, 200)
      \multiput(0,0)(35, 0){6}{
        \put(-25, 45){\vector(1, 0){25}}
        \put(-25, 100){\vector(1, 0){25}}
      }
      \multiput(0,0)(35, 0){5}{
        \put(0, 0){
          \put(0, 85){\framebox(10, 30){}}
          \put(0, 30){\framebox(10, 30){}}
          \multiput(0,0)(0, 5){4}{
            \put(5, 7){\line(0, 0){2}}
            \put(5, 60){\line(0, 0){2}}
            \put(5, 115){\line(0, 0){2}}
          }
          \put(5, 30){\vector(0, 0){0.1}}
          \put(5, 85){\vector(0, 0){0.1}}
          \put(5, 138){\vector(0, 0){0.1}}
        }
      }
      \put(-2, 0){\makebox{$x_{i-2}$}}
      \put(33, 0){\makebox{$x_{i-1}$}}
      \put(71, 0){\makebox{$x_{i}$}}
      \put(103, 0){\makebox{$x_{i+1}$}}
      \put(138, 0){\makebox{$x_{i+2}$}}
      \put(-2, 142){\makebox{$y_{i-2}$}}
      \put(33, 142){\makebox{$y_{i-1}$}}
      \put(71, 142){\makebox{$y_{i}$}}
      \put(103, 142){\makebox{$y_{i+1}$}}
      \put(138, 142){\makebox{$y_{i+2}$}}
    \end{picture}
  \end{center}
  \caption{Regularized multilayer RNN. Dashed arrows indicate connections with applied dropout, while
  solid lines indicate connections where dropout is not applied.}
\end{figure}



Dropout removes part of information, and units have to become
more robust while performing intermediate input-intermediate output mapping. Simultaneously we don't want to erase
entire information. Especially we would like to facilitate memory about event that happened
long time ago. Figure \ref{fig:flow} shows a flow of an exemplary information from the 
event $x_{i-2}$ to the prediction in the step $i+2$. We can see that information is influenced
with dropout only $L + 1$ times, and it is independent of how far in past event occurred. All
the previous regularization techniques were constraining recurrent connections, which 
effectively exponentially fast ``blurred'' information about the past. 





\begin{figure}
  \begin{center}
    \begin{picture}(150, 200)
      \multiput(0,0)(35, 0){6}{
        \put(-25, 45){\vector(1, 0){25}}
        \put(-25, 100){\vector(1, 0){25}}
      }
      \multiput(0,0)(35, 0){5}{
        \put(0, 0){
          \put(0, 85){\framebox(10, 30){}}
          \put(0, 30){\framebox(10, 30){}}
          \multiput(0,0)(0, 5){4}{
            \put(5, 7){\line(0, 0){2}}
            \put(5, 60){\line(0, 0){2}}
            \put(5, 115){\line(0, 0){2}}
          }
          \put(5, 30){\vector(0, 0){0.1}}
          \put(5, 85){\vector(0, 0){0.1}}
          \put(5, 138){\vector(0, 0){0.1}}
        }
      }
      \put(-2, 0){\makebox{$x_{i-2}$}}
      \put(33, 0){\makebox{$x_{i-1}$}}
      \put(71, 0){\makebox{$x_{i}$}}
      \put(103, 0){\makebox{$x_{i+1}$}}
      \put(138, 0){\makebox{$x_{i+2}$}}
      \put(-2, 142){\makebox{$y_{i-2}$}}
      \put(33, 142){\makebox{$y_{i-1}$}}
      \put(71, 142){\makebox{$y_{i}$}}
      \put(103, 142){\makebox{$y_{i+1}$}}
      \put(138, 142){\makebox{$y_{i+2}$}}

       
      {\linethickness{0.6mm}
        \put(5, 7){\line(0, 0){38}}
        \put(4, 45){\line(1, 0){105}}
        \put(110, 44){\line(0, 0){56}}
        \put(109, 100){\line(1, 0){35}}
        \put(145, 99){\line(0, 0){35}}
      }
    \end{picture}
  \end{center}
  \caption{Thick line indicates an exemplary information flow in RNN. Information flow line is crossed $L + 1$ times, where $L$ is depth of network.}
  \label{fig:flow}
\end{figure}


\section{Experiments}

We present here results in various domains (1) language modeling \ref{sec:lang}, 
(2) speech recognition \ref{sec:speech}, and (3) machine translation \ref{sec:trans}.

\subsection{Language modeling}
\label{sec:lang}

We have conducted word-level prediction experiments on Penn tree bank (PTB) dataset. 
This dataset consists of $929$k training examples, $73$k validation examples, 
and $82$k test examples. It has $10$k words in vocabulary. 


Our model is a two layer LSTM network, with 650 units initialized uniformly in
$[-0.05, 0.05]$. We apply $50\%$ of dropout on non-recurrent connections. We train for
$39$ epochs, starting with learning rate $1$, and after $6$ epochs we decrease it by $1.2$ 
in every epoch. We unroll RNN for $35$ steps, and clip gradients at $5$. We set mini-batch
to $20$. Table \ref{tab:ptb} shows our results. 

\begin{table}[t]
  \small
  \centering
  \renewcommand{\arraystretch}{1.15}
  \begin{tabular}{lll}
    \hline
     Model & Validation set & Test set \\
    \hline
    \multicolumn{3}{c}{A single model} \\
    \hline
    Previous state-of-the-art \tablefootnote{\cite{pascanu2013construct}} & & 107.5 \\
    Regularized LSTM & {\bf 86.2} & {\bf 82.7} \\
    \hline
    \multicolumn{3}{c}{Model averaging} \\
    \hline
    Previous state-of-the-art \tablefootnote{\cite{mikolov2012statistical}} & 83.5\tablefootnote{Weight of individual models are tuned to minimize this score. This few parameters are fit on this validation set, which is not completely fair.} & 89.4 \\
    2 regularized LSTMs & 80.6 & 77.0 \\
    5 regularized LSTMs & & \\
    10 regularized LSTMs & & \\
    \hline
  \end{tabular}
  \caption{Word-level perplexity on Penn-tree-bank dataset.}
  \label{tab:ptb}
\end{table}

\subsection{Speech recognition}
\label{sec:speech}

\subsection{Machine translation}
\label{sec:trans}

\section{Discussion}

\bibliography{bibliography}
\bibliographystyle{icml2014}

\end{document} 

