% Use the following line  only  if you're still using LaTeX 2.09.
%\documentstyle[icml2014,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}
% use Times
\usepackage{times}
 % For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 
\usepackage{footnote}
\usepackage{amsfonts}

% For citations
\usepackage{natbib}
\usepackage{comment}
\usepackage{multirow}


% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}

\setlength{\abovedisplayskip}{0cm}
\setlength{\belowdisplayskip}{0cm}

\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{0.5ex}{0.3ex}
\titlespacing{\subsection}{0pt}{0.2ex}{0ex}
\titlespacing{\subsubsection}{0pt}{0.1ex}{0ex}

\newcommand{\startcompact}[1]{\par\vspace{-0.75em}\begin{#1}%
  \allowdisplaybreaks\ignorespaces}

\newcommand{\stopcompact}[1]{\end{#1}\ignorespaces}

\usepackage{paralist}

\makeatletter
\ifcase \@ptsize \relax% 10pt
  \newcommand{\miniscule}{\@setfontsize\miniscule{4}{5}}% \tiny: 5/6
\or% 11pt
  \newcommand{\miniscule}{\@setfontsize\miniscule{5}{6}}% \tiny: 6/7
\or% 12pt
  \newcommand{\miniscule}{\@setfontsize\miniscule{5}{6}}% \tiny: 6/7
\fi
\makeatother

\newcommand {\aplt} {\ {\raise-.5ex\hbox{$\buildrel<\over\sim$}}\ }

\newcommand{\eqn}[1]{Eqn.~\ref{eqn:#1}}
\newcommand{\fig}[1]{Fig.~\ref{fig:#1}}
\newcommand{\tab}[1]{Table~\ref{tab:#1}}
\newcommand{\secc}[1]{Section~\ref{sec:#1}}
\def\etal{{\textit{et~al.~}}}
\newcommand{\BigO}[1]{\ensuremath{\operatorname{O}\left(#1\right)}}
\usepackage[symbol*]{footmisc}

\DefineFNsymbolsTM{myfnsymbols}{% def. from footmisc.sty "bringhurst" symbols
  \textasteriskcentered *
  \textdagger    \dagger
  \textdaggerdbl \ddagger
  \textsection   \mathsection
  \textbardbl    \|%
  \textparagraph \mathparagraph
}%


% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
% \usepackage[nohyperref]{icml2014} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
%\usepackage{icml2014} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
\usepackage[accepted]{icml2014}

\begin{document} 

\twocolumn[
\icmltitle{Recurrent neural network regularization}

\icmlauthor{Wojciech Zaremba}{woj.zaremba@gmail.com}
\vskip -0.03in
\icmladdress{Google \& New York University}
\vskip -0.08in
\icmlauthor{Ilya Sutskever}{ilyasu@google.com}
\vskip -0.03in
\icmladdress{Google}
\vskip -0.08in
\icmlauthor{Oriol Vinyals}{vinyals@google.com}
\vskip -0.03in
\icmladdress{Google}


\icmlkeywords{natual language processing, recurrent neural networks, language model, LSTMs, speech recognition, machine translation}

\vskip 0.3in
]

\begin{abstract} 
  We present a simple regularization technique for Recurrent Neural
  Networks (RNNs) with Long Short-Term Memory (LSTM) units.  The
  technique is based on dropout and gives a large reduction in
  overfitting.  We show that it is useful in a variety of sequence
  modeling problems that include language modeling, speech
  recognition, and machine translation.
\end{abstract} 

\section{Introduction}

RNNs are a neural sequence model that achieves state of the art
performance on important tasks that include language modeling
\cite{mikolov2012statistical}, speech recognition
\cite{graves2013speech}, and machine translation
\cite{cho2014learning}.  It is known that successful applications of
neural networks require good regularization. Unfortunately, dropout
\cite{srivastava2013improving},  the most powerful regularization method
for feedforward neural networks, does not work well for
RNNs. As a result, practical applications often
use RNNs that are too small, as large RNNs tend to overfit.  To date,
existing regularization methods give relatively small improvements for
RNNs \cite{graves2013generating}.
% , although deterministic approximations to
% dropout may work well with RNNs \cite{bayer2013fast,wang2014fast}.  
In this work, we show how to correctly apply dropout to LSTMs, and
demonstrate that it successfully reduces overfitting on three different problems. 

\section{Related work}

Dropout \cite{srivastava2013improving} is a recently introduced
regularization method that has enjoyed a lot of success with
feed-forward neural networks.  While there has been a lot of work
extending dropout \cite{wang2013fast,wan2013regularization}, there has
been relatively little research in applying dropout to RNNs. The only
paper on the topic is \citet{bayer2013fast} which focuses on
``marginalized dropout'' (from \citet{wang2013fast}) which is a
noiseless deterministic approximation to conventional dropout.
\citet{bayer2013fast} claims that conventional dropout cannot be used
with RNNs because the recurrence causes large variance which hurts
learning. In this work, we show how to overcome the above problem, by
applying dropout to a specific subset of the RNNs connections.  As a
result, we are able to greatly reduce overfitting.

There has been extensive work on RNNs for language modeling
\cite{mikolov2012statistical, sutskever2013training}. Moreover, there
have been a number of architectural variations on the RNN that are
work better learning with long term dependencies
\cite{hochreiter1997long, graves2009novel, cho2014learning,
  jaeger2007optimization, koutnik2014clockwork}.  In this work, we
show how to correctly apply dropout to the LSTM, which is the most
commonly-used RNN variant,  although it is likely that this application
of dropout extends to other RNNs.

In this paper, we consider the following RNN tasks: language modeling,
speech recognition, and machine translation.  Language modeling is the
first task where RNNs achieved substantial success
\cite{mikolov2010recurrent, mikolov2011strategies,
  pascanu2013construct}.  RNNs have also been successfully used for
speech recognition \cite{robinson1996use, graves2013speech} and have
recently been applied to machine translation, where they are typically
used for language modeling, re-ranking, or phrase modeling
\cite{cho2014learning,chow1987byblos,mikolov2013exploiting}.

\section{Regularizing RNN with LSTM cells}

In this section we describe the deep LSTM \ref{sec:lstm}. Next, 
we show how to regularize them \ref{sec:reg}, and we provide an intuition
for why our regularization scheme works.

We use subscript to denote a timestep and superscript to denote 
a layer number.  All our states are $n$-dimensional.  Let $h^l_k
\in \mathbb{R}^{n}$ be a hidden state in layer $l$ in step
$k$. Moreover, let $T_{n,m}:\mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$
be a linear transform and with a bias ($Wx + b$ for some $W$ and $b$).
Let $\odot$ be a element-wise multiplication and let $h^0_k$ be an
input word vector.  We use the activations $h^{L}_k$ to predict $y_k$,
since $L$ is the number of layers in our deep LSTM.

\subsection{Long-short term memory units}
\label{sec:lstm}

The RNNs dynamics can be described in terms of deterministic transitions
from previous hidden states to the hidden states in the next step. 
In all following equations, lower index $t$ denotes time indexing, while upper index $l$ represents stacked layers indexing.
The transition is a function
\begin{align*}
  &\text{RNN} : h^{l-1}_t, h^l_{t-1} \rightarrow h^l_t
\end{align*}

For classical RNNs, this function is given by
\begin{align*}
  h^l_t = f(T_{n,n}h^{l-1}_t + T_{n,n}h^l_{t-1}) \text{, where $f \in \{\mathrm{sigm}, \tanh\}$ }
\end{align*}

The LSTM has relatively complicated dynamics that make it easy to
``memorize'' information for extended number of time steps.  The
``long term'' memory is stored in a vector of \emph{memory cells}
$c^l_k \in \mathbb{R}^n$.  Although there are many LSTM architectures
that differ in their connectivity sturcutre and activation functions,
all LSTM architectures have explicit memory cells that make it easy to
store information for extended periods of time.  The LSTM can decide
to overwrite this information, retrieve it, or keep it for the next time
step.  The LSTM architecture used in our experiments is given by the
following equations \cite{graves2013speech}:
\begin{align*}
&\text{LSTM} : h^{l-1}_t, h^l_{t-1}, c^l_{t - 1} \rightarrow h^l_t, c^l_t\\
&\begin{pmatrix}i\\f\\o\\g\end{pmatrix} =
  \begin{pmatrix}\mathrm{sigm}\\\mathrm{sigm}\\\mathrm{sigm}\\\tanh\end{pmatrix}
  T_{2n,4n}\begin{pmatrix}h^{l - 1}_t\\h^l_{t-1}\end{pmatrix}\\
&c^l_t = f \odot c^l_{t-1} + i \odot g\\
&h^l_t = o \odot \tanh(c^l_t)\\
\end{align*}
In these equations, $\mathrm{sigm}$ and $\tanh$ are applied
element-wise. Diagram \ref{fig:lstm} illustrates the LSTM
equations.


\begin{figure}
  \begin{center}
    \begin{picture}(200, 130)
      \put(0, 0){\framebox(180, 100){}}
      \put(90, 50){\circle{16}}
      \put(86.5, 48){$\mathbf c_t$}
      \put(84, 60){{\scriptsize Cell}}

      \put(90, 32){\circle{6.5}}
      \put(87.25, 30.5){{\tiny $\times$}}

      \put(90, 12){\circle{16}}
      \put(86.5, 10){{\small $f$}}
      \put(100, 10){{\scriptsize Forget gate}}

      \put(90, 20){\vector(0, 0){8}}

      \put(85, 44){\vector(2, 3){0}}
      \qbezier(86, 32)(80, 36.5)(83, 41)

      \qbezier(96, 34)(100, 36.5)(95.5, 43.5)
      \put(93.5, 31.5){\vector(-1, -1){0}}
      
      \put(82, -8){\vector(1, 2){6}}
      \put(72.5, -13){{\small $h_{t-1}^{l}$}}
      \put(98, -8){\vector(-1, 2){6}}
      \put(99, -13){{\small $h_{t}^{l-1}$}}

      \put(30, 87){\circle{16}}
      \put(28, 85){{\small $i$}}
      \put(6, 85){{\scriptsize $\begin{matrix}\text{Input}\\\text{gate}\end{matrix}$}}

      \put(21.5, 105){\vector(1, -2){5}}
      \put(12.5, 108){{\small $h_{t-1}^{l}$}}
      \put(39.5, 107){\vector(-1, -2){6}}
      \put(37, 108){{\small $h_{t}^{l-1}$}}

      \put(147, 87){\circle{16}}
      \put(144.5, 85){{\small $o$}}
      \put(156, 85){{\scriptsize $\begin{matrix}\text{Output}\\\text{gate}\end{matrix}$}}
        
      \put(138.5, 105){\vector(1, -2){5}}
      \put(129.5, 108){{\small $h_{t-1}^{l}$}}
      \put(156.5, 107){\vector(-1, -2){6}}
      \put(154, 108){{\small $h_{t}^{l-1}$}}

      \put(17, 50){\circle{16}}
      \put(15, 48){{\small $g$}}
      \put(1, 28){{\scriptsize $\begin{matrix}\text{Input}\\\text{modulation}\\\text{gate}\end{matrix}$}}

      \put(53.5, 50){\circle{6.5}}
      \put(50.75, 48.5){{\tiny $\times$}}

      \put(57, 50){\vector(1, 0){25}}
      \put(25, 50){\vector(1, 0){25}}
      \put(35, 80){\vector(2, -3){17.5}}

      \put(147, 50){\circle{6.5}}
      \put(144.25, 48.5){{\tiny $\times$}}
      \put(98, 50){\vector(1, 0){45.25}}
      \put(150.5, 50){\vector(1, 0){38}}

      \put(147, 79){\vector(0, -1){25.5}}
      \put(190, 47){${\mathbf h^l_t}$}


      \put(-20, 40){{\small $h_{t-1}^{l}$}}
      \put(-20, 56){{\small $h_{t}^{l-1}$}}
      \put(-10, 44){\vector(4, 1){19}}
      \put(-10, 58){\vector(4, -1){19}}


    \end{picture}
  \end{center}
  \caption{Graphical representation of LSTM memory cells used in this paper (there are minor differences in compare to \cite{graves2013generating}.}
  \label{fig:lstm}
\end{figure}


\subsection{Regularization with Dropout} 
\label{sec:reg}

The main contribution of this paper is a demonstration that
correctly-used can dropout greatly reduce overfitting in LSTMs.  To be
effective, dropout must be placed on the non-recurrent connections
\ref{fig:reg}.  The following equation describes it more precisely,
where ${\bf D}$ is the dropout operator that sets a random subset of
its argument to zero:

\begin{align*}
&\begin{pmatrix}i\\f\\o\\g\end{pmatrix} =
  \begin{pmatrix}\mathrm{sigm}\\\mathrm{sigm}\\\mathrm{sigm}\\\tanh\end{pmatrix}
  T_{2n,4n}\begin{pmatrix}{\bf D}(h^{l - 1}_t)\\h^l_{t-1}\end{pmatrix}\\
&c^l_t = f \odot c^l_{t-1} + i \odot g\\
&h^l_t = o \odot \tanh(c^l_t)\\
\end{align*}


\begin{figure}
  \begin{center}
    \begin{picture}(150, 200)
      \multiput(0,0)(35, 0){6}{
        \put(-25, 45){\vector(1, 0){25}}
        \put(-25, 100){\vector(1, 0){25}}
      }
      \multiput(0,0)(35, 0){5}{
        \put(0, 0){
          \put(0, 85){\framebox(10, 30){}}
          \put(0, 30){\framebox(10, 30){}}
          \multiput(0,0)(0, 5){4}{
            \put(5, 7){\line(0, 0){2}}
            \put(5, 60){\line(0, 0){2}}
            \put(5, 115){\line(0, 0){2}}
          }
          \put(5, 30){\vector(0, 0){0.1}}
          \put(5, 85){\vector(0, 0){0.1}}
          \put(5, 138){\vector(0, 0){0.1}}
        }
      }
      \put(-2, 0){\makebox{$x_{t-2}$}}
      \put(33, 0){\makebox{$x_{t-1}$}}
      \put(71, 0){\makebox{$x_{t}$}}
      \put(103, 0){\makebox{$x_{t+1}$}}
      \put(138, 0){\makebox{$x_{t+2}$}}
      \put(-2, 142){\makebox{$y_{t-2}$}}
      \put(33, 142){\makebox{$y_{t-1}$}}
      \put(71, 142){\makebox{$y_{t}$}}
      \put(103, 142){\makebox{$y_{t+1}$}}
      \put(138, 142){\makebox{$y_{t+2}$}}
    \end{picture}
  \end{center}
  \caption{Regularized multilayer RNN. Dashed arrows indicate connections with applied dropout, while
  solid lines indicate connections where dropout is not applied.}
  \label{fig:reg}
\end{figure}

We implement dropout for LSTMs as follows.  The dropout operator
corrupts the information carried by the units, which forces them to
perform their intermediate computations more robustly. At the same
time, we do not want to erase all the information. We would especially
like the units to remember events that occurred many timeseps in the
past. Figure \ref{fig:flow} shows how information could flow from an
event that occurred at $x_{t-2}$ to the prediction in timestep $t+2$
in our implementation of dropout. We can see that the information is
corrupted by the dropout operator exactly $L + 1$ times, and this
number is independent of the number of timesteps traversed by the
information.  A naive application of dropout would perturb the
recurrent connections or the recurrent hidden state, which would make
it difficult LSTM's to store information for many timesteps.  By not
using dropout on the recurrent connections, the LSTM is able to get
most of the benefit of dropout without sacrifising its valuable
ability to learn and store information for long periods of time.


\begin{figure}
  \begin{center}
    \begin{picture}(150, 200)
      \multiput(0,0)(35, 0){6}{
        \put(-25, 45){\vector(1, 0){25}}
        \put(-25, 100){\vector(1, 0){25}}
      }
      \multiput(0,0)(35, 0){5}{
        \put(0, 0){
          \put(0, 85){\framebox(10, 30){}}
          \put(0, 30){\framebox(10, 30){}}
          \multiput(0,0)(0, 5){4}{
            \put(5, 7){\line(0, 0){2}}
            \put(5, 60){\line(0, 0){2}}
            \put(5, 115){\line(0, 0){2}}
          }
          \put(5, 30){\vector(0, 0){0.1}}
          \put(5, 85){\vector(0, 0){0.1}}
          \put(5, 138){\vector(0, 0){0.1}}
        }
      }
      \put(-2, 0){\makebox{$x_{t-2}$}}
      \put(33, 0){\makebox{$x_{t-1}$}}
      \put(71, 0){\makebox{$x_{t}$}}
      \put(103, 0){\makebox{$x_{t+1}$}}
      \put(138, 0){\makebox{$x_{t+2}$}}
      \put(-2, 142){\makebox{$y_{t-2}$}}
      \put(33, 142){\makebox{$y_{t-1}$}}
      \put(71, 142){\makebox{$y_{t}$}}
      \put(103, 142){\makebox{$y_{t+1}$}}
      \put(138, 142){\makebox{$y_{t+2}$}}

       
      {\linethickness{0.6mm}
        \put(5, 7){\line(0, 0){38}}
        \put(4, 45){\line(1, 0){105}}
        \put(110, 44){\line(0, 0){56}}
        \put(109, 100){\line(1, 0){35}}
        \put(145, 99){\line(0, 0){35}}
      }
    \end{picture}
  \end{center}
  \caption{Thick lines show a how information flows in the LSTM. The
    information is affected by dropout $L + 1$ times, where $L$ is
    depth of network.}
  \label{fig:flow}
\end{figure}


\section{Experiments}

We present here results in there domains: language modeling \ref{sec:lang}, 
speech recognition \ref{sec:speech}, and machine translation \ref{sec:trans}.

\subsection{Language modeling}
\label{sec:lang}

We have conducted word-level prediction experiments on Penn tree bank
(PTB) dataset \cite{marcus1993building}.  This dataset consists of
$929$k training words, $73$k validation words, and $82$k test
words. It has $10$k words in vocabulary. We have trained regularized LSTMs of two
sizes; we denote them the medium LSTM and the large LSTM.  Both LSTMs
have two layers, and are unrolled for $35$ steps. We initilize hidden states to zeros.
We pass final hidden states after unrollment as a initilia hidden state of the next unrollment.
We set mini-batch to
20.

\begin{table}[t]
  \small
  \centering
  \renewcommand{\arraystretch}{1.15}
  \begin{tabular}{lll}
    \hline
     Model & Validation set & Test set \\
    \hline
    \multicolumn{3}{c}{A single model} \\
    \hline
    \citet{pascanu2013construct} & & 107.5 \\
    \citet{chenglanguage} & & 100.0 \\
    non-regularized LSTM & 120.7 & 114.5 \\
    Medium regularized LSTM & 86.2 & 82.7 \\
    Large regularized LSTM & 82.2 & {\bf 78.4} \\
    \hline
    \multicolumn{3}{c}{Model averaging} \\
    \hline
    \citet{mikolov2012statistical} & 83.5 & 89.4 \\
    \citet{chenglanguage} & & 80.6 \\
    2 non-regularized LSTMs & 100.4 & 96.1 \\
    5 non-regularized LSTMs & 87.9 & 84.1 \\
    10 non-regularized LSTMs & 83.5 & 80.0 \\
    2 medium regularized LSTMs & 80.6 & 77.0 \\
    5 medium regularized LSTMs & 76.7 & 73.3 \\
    10 medium regularized LSTMs & 75.2 & 72.0 \\
    2 large regularized LSTMs & 76.9 & 73.6 \\
    10 large regularized LSTMs & 72.8 & 69.5 \\
    38 large regularized LSTMs & 71.9 & {\bf 68.7} \\
    \hline
    \multicolumn{3}{c}{Model averaging with dynamic RNNs} \\
    \hline
    \citet{mikolov2012context} & & 72.9 \\
    \hline
  \end{tabular}
  \caption{Word-level perplexity on Penn-tree-bank dataset.}
  \label{tab:ptb}
\end{table}

\begin{figure}
\line(1,0){235}

  {\footnotesize
  \textit{the meaning of life is} that only if an end would be of the whole supplier. widespread rules are regarded as the companies of refuses to deliver. in balance of the nation 's information and loan growth associated with the carrier thrifts are in the process of slowing the seed and commercial paper.}
\line(1,0){235}

  {\footnotesize
\textit{the meaning of life is} nearly in the first several months before the government was addressing such a move as president and chief executive of the nation past from a national commitment to curb grounds. meanwhile the government invests overcapacity that criticism and in the outer reversal of small-town america.}

\line(1,0){235}
  \caption{Some interesting samples drawn from large regularized model conditioned on ``The meaning of life is''. We have removed ``unk'', ``N'', ``\$'' from possible outcomes.}
  \label{fig:meaning}
\end{figure}

The medium-sized LSTM has $650$ units per layer
and its parameters are initialized uniformly in $[-0.05,
  0.05]$. As described earlier, we apply $50\%$ dropout on the non-recurrent connections. We
train the LSTM for $39$ epochs with learning rate of $1$, and after
$6$ epochs we decrease it by a factor of $1.2$ in each epoch. We
clip the norm of the gradients (normalized by minibatch size) at $5$. 
Training of such network on NVIDIA K20 takes about a half a day.

The large LSTM has $1500$ units per layer and its parameters are
initialized uniformly in $[-0.04, 0.04]$. We apply $65\%$ dropout on
the non-recurrent connections. We train the model for $55$ epochs with
learning rate of $1$; we start decreasing the learning rate by a
factor of $1.15$ in each epoch after $14$ epochs. We clip the norm of
the gradients (normalized by minibatch size) at $10$. Training of such
network on NVIDIA K20 takes a day.

Moreover, for a comparison we have trained a non-regularized network. 
We optimized parameters to get the best validation performance.
Lack of regularization
puts constrains on size of the network, and we had to considered quite small network as a big one overfits. 
Our best performing non-regularized model has $200$ units per layer, and two layers. Weights are initialized
uniformaly in $[-0.1, 0.1]$.
We train it for $4$ epochs 
with learning rate $1$, and then we descrese learning rate by $2$ in each epoch until $13$ epoch.
Size of minibatch for this network is $20$, and we unroll it for $20$ steps. Training of such network
on NVIDIA K20 takes 2-3 hours.

Table \ref{tab:ptb} compares previous results with our LSTMs, and
figure \ref{fig:meaning} shows samples draw from a single large size
regularized model.



\subsection{Speech recognition}
\label{sec:speech}

Deep Neural Networks have been used for acoustic modeling for more
than half a century (the reader is encouraged to read
\citet{BourlardASR} for a good review). Acoustic modeling is a key
component in mapping acoustic signals to sequences of words, as it
models $p(s_t|X)$ where $s_t$ is the phonetic state at time t, and $X$
the acoustic observation. Recent work has shown that LSTMs are very
capable of excellent acoustic modeling \cite{sak2014speech}, and that
relatively small LSTMs (in terms of their number of parameters) can
overfit easily. A useful metric for measuring acoustic models is frame
accuracy, which is measured on each $s_t$ prediction for all timesteps
$t$. Generally, this metric correlates with the actual metric of
interest, the Word Error Rate (WER). However, since computing WER
involves using a language model and tuning the decoding parameters for
every change in the acoustic model, we decided to report frame
accuracy in these experiments. Table~\ref{tab:speech} clearly shows
that dropout improves the frame accuracy of the LSTM. Not
surprisingly, the training frame accuracy drops due to the noise added
during training, but as is often the case with dropout, this yields
models that generalize better to unseen data. Note that the testing
set is easier than the training set, given its higher accuracy.  We
report the performance of an LSTM on an internal Google Icelandic
Speech dataset, which is relatively small (93k utterances), so
overfitting is a greater concern.

\begin{table}[t]
  \small
  \centering
  \renewcommand{\arraystretch}{1.15}
  \begin{tabular}{lll}
    \hline
     Model & Training set & Validation set \\
    \hline
    Non-regularized LSTM & 71.6 & 68.9 \\
    Regularized LSTM & 69.4 & {\bf 70.5} \\
    \hline
  \end{tabular}
  \caption{Frame-level accuracy on Icelandic Speech Dataset. Training data contains 93k utterances.}
  \label{tab:speech}
\end{table}


\subsection{Machine translation}
\label{sec:trans}

We report the performance of the LSTM on a machine translation task.
We formulate the translation task as a language modelling task, where
the LSTM is trained to assign high probability to the correct
translation given a source sentence.  Thus, the LSTM is trained on
sequences of the form \texttt{(source sentence, target sentence)}
\cite{mt_paper,cho2014learning}. We compute the translations using a
simple beam search with a beam of size 12 that finds the most likely
sequence of words given the input sequence of words.  We ran this
experiment on the WMT'14 English to French dataset, on the
``selected'' subset from \citet{wmt_joint} which has 340M French words
and 304M English words.  Our LSTM has 4 hidden layers, where both the
layers and the word embeddings are 1000-dimensional, and where the
English vocabulary has 160,000 words and the French vocabulary has
80,000 words. We found the optimal dropout probability to be 0.2.
Table \ref{tab:mt} shows the comparative performance of both LSTMs.
While this particular LSTM does not beat a standard SMT system
\cite{lium}, the result clearly shows that dropout improves the
translation performance of the LSTM.

\begin{table}[t]
  \small
  \centering
  \renewcommand{\arraystretch}{1.15}
  \begin{tabular}{lll}
    \hline
     Model & Test perplexity & Test BELU score \\
    \hline
    Non-regularized LSTM & 5.8 & 25.9 \\
    Regularized LSTM & 5.0 &  29.03 \\
    \hline
    LIUM system &  &  33.30 \\
    \hline
  \end{tabular}
  \caption{Results on the English to French translation task. }
  \label{tab:mt}
\end{table}



\section{Discussion}

We presented a simple application of dropout to LSTMs that resulted in
large performance boosts on many separate domains.  Our results make dropout
useful for RNNs, and our results suggest that this type of dropout could improve
performance on a wide variety of applications.




\bibliography{bibliography}
\bibliographystyle{icml2014}

\end{document} 

